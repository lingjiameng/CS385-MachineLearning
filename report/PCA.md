PCA (Principle component analysis) 主成分 分析是一种数据降维的方法，该方法的核心效果是特征**方差最大化**和特征之间**去相关**。对于 LinearPCA 来说，去相关只能解除线性相关性，无法处理高阶的相关性，对于高阶相关的数据，我们可以采用核技巧即 KernelPCA。

PCA是无标签的

## PCA的分析

### 数学推理

概率论的方差，协方差，线性代数的特征值和特征向量，奇异值分解。以及对线性变换的理解

假设我们有如下的信息
$$
\begin{align}
X \in \mathcal{R}^{n \times p} <== \mbox{数据集(均值为零)，n个样本，每个样本p个特征}

\end{align}
$$
我们定义如下一个线性变换矩阵 
$$
\begin{align}
w \in \mathcal{R}^{p \times k} <== \mbox{线性变换,将n维向量投影为k维向量}
\end{align}
$$
我们的目标是是的投影后的数据方差最大化，同时投影后的数据特征之间无关
$$
T = Xw \in \mathcal{R}^{n \times k} <== \mbox{投影后的结果}
$$
对于投影后的数据方差，(均值为零) 易知：
$$
T_{cov} = \frac{1}{n}T^T T
$$
所以我们的目标函数
$$
argmax_w\ T_{cov} 
$$



## PCA 和 LDA 的对比

PCA主成分分析,主要效果是是**特征方差最大化**并且**去除特征间的相关性**

计算时使用了瑞利熵的技巧



PCA是无监督的,基于方差越大信息越多的原理和假设.

最大化特征方差, 最小化特征间方差. 特征间的信息重复率降低



LDA线性判别分析,主要效果是改变数据分布, 从而**最大化类间方差**,**最小化类内方差**

计算时使用了广义瑞利熵的技巧



LDA效果比PCA好的原因,不是简单的特征最大化

<https://blog.csdn.net/asd136912/article/details/78757482>